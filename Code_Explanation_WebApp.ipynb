{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-ghosh2992/Authentication/blob/main/Code_Explanation_WebApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nho9cCX7mMRV"
      },
      "source": [
        "# Project By team Byteblaser\n",
        "Developder by\n",
        "\n",
        "*   Aditya Ghosh\n",
        "*   Ankan Misra\n",
        "*   Suman Jain\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is a python based program developd in the Ideathon'24 organised by NSEC , Kolkata.\n",
        "\n",
        "Project 1 Name :- Code Explanation\n",
        "\n",
        "Used Case :- Its simplify any code to a laymans Language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R3rBEO0mRx9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4ILymDJuoMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c276f9-6bfc-4a58-9ad6-2169f715e08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as palm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrLD58wovyvk"
      },
      "outputs": [],
      "source": [
        "palm.configure(api_key='AIzaSyASboLpJbVoVifYDJwOX5-z7Ggtr9OgzPg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WfMIig2HO8iV",
        "outputId": "23b00b5e-07d3-43c6-f18f-515e1c5debaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "list(palm.list_models())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "2c7284c4-8f51-422e-c6ca-0723514e1b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ],
      "source": [
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "fcc2b791-cc42-4579-ca60-06a8cd8dd5c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNBhUFz6m3C"
      },
      "source": [
        "**Simple Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "k86thLvS7qdt",
        "outputId": "98b755ba-e6b5-4ec1-a0f0-6a3ff5dda664"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'palm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0eaecf957f41>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprompt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Why Generative AI growing too much? Provide answers in bullain format in clear and concise maner\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m completion = palm.generate_text(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'palm' is not defined"
          ]
        }
      ],
      "source": [
        "# Set Your Input Text\n",
        "prompt1 = \"Why is the Sky blue?\"\n",
        "prompt2 = \"Why Generative AI growing too much? Provide answers in bullain format in clear and concise maner\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt2,\n",
        "    temperature = 0,\n",
        "    # temperature =0 >> more deterministic results // temperature = 1 >> more randomness\n",
        "    max_output_tokens = 100\n",
        ")\n",
        "print(completion.result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyADRKKF-Sig"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt):\n",
        "  completion = palm.generate_text(\n",
        "      model=model,\n",
        "      prompt=prompt,\n",
        "      temperature=0,\n",
        "      # The maximum length of the response\n",
        "      max_output_tokens=500\n",
        "  )\n",
        "  response = completion.result\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEab6Q9_9MVT"
      },
      "source": [
        "**Simple Prompting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOSv-jqL9Prh"
      },
      "outputs": [],
      "source": [
        "input_code = f\"\"\"\n",
        "x = [1,2,3,4,5]\n",
        "y = [i*i for i in x if i%2==0]\n",
        "print(y)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuE4xLPj9jQW",
        "outputId": "ad6afa8f-7cb9-42b7-b3a6-4220c7560d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "your Task is to act as a Python Code Explainer.\n",
            "I'll give you Code Snippet.\n",
            "Your Job is to explain the Code Snippet step-by-step.\n",
            "Also, compute the final output of the code.\n",
            "Code Snippet is shared below, delimited with triple backticks.\n",
            "```\n",
            "\n",
            "x = [1,2,3,4,5]\n",
            "y = [i*i for i in x if i%2==0]\n",
            "print(y)\n",
            "\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "your Task is to act as a Python Code Explainer.\n",
        "I'll give you Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Also, compute the final output of the code.\n",
        "Code Snippet is shared below, delimited with triple backticks.\n",
        "```\n",
        "{input_code}\n",
        "```\n",
        "\n",
        "\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "4XjOGzqv-7NJ",
        "outputId": "4ee6fa97-489c-429b-daa2-0ab3239467f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "# Create a list x with numbers from 1 to 5\n",
            "x = [1,2,3,4,5]\n",
            "\n",
            "# Create a new list y that contains the squares of the even numbers in x\n",
            "y = [i*i for i in x if i%2==0]\n",
            "\n",
            "# Print the list y\n",
            "print(y)\n",
            "\n",
            "```\n",
            "\n",
            "Output:\n",
            "```\n",
            "[4, 16]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(get_completion(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyjKqv2QYA_r"
      },
      "source": [
        "Sample Test Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEwKUkI5ETeH",
        "outputId": "efba3461-d7dc-4a8d-b452-e814daaaa424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LowHighLow\n"
          ]
        }
      ],
      "source": [
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5q8x-uhD0MS"
      },
      "outputs": [],
      "source": [
        "input_code = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA4cwQv-EgQN",
        "outputId": "6e48364b-87e2-4802-9534-ebc936a70c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your task is to act as a Python Code Explainer.\n",
            "I'll give you a Code Snippet.\n",
            "Your job is to explain the Code Snippet Step-by-Step.\n",
            "Also, compute the final output of the code.\n",
            "Code Snippet is shared below, delimited with triple backticks:\n",
            "\n",
            "```\n",
            "\n",
            "def my_func(x):\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "print(result)\n",
            "\n",
            "\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to act as a Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your job is to explain the Code Snippet Step-by-Step.\n",
        "Also, compute the final output of the code.\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "\n",
        "```\n",
        "{input_code}\n",
        "```\n",
        "\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "nldXvoQ3FNWd",
        "outputId": "00da50fe-9734-48fe-a111-d462153c7dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-by-Step:\n",
            "1. The function `my_func` takes an integer as input and returns `\"High\"` if the input is greater than 5, otherwise it returns `\"Low\"`.\n",
            "2. The variable `result` is assigned the value of `my_func(4)` + `my_func(6)` + `my_func(4)`.\n",
            "3. The value of `result` is printed to the console, which is `HighLowLow`.\n",
            "\n",
            "Final output:\n",
            "```\n",
            "HighLowLow\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(get_completion(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqQGl7qQFgi9"
      },
      "source": [
        "**Effective Prompting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL4f0x46FkD4"
      },
      "outputs": [],
      "source": [
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "  global x\n",
        "  x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xudiEApxG0Vc"
      },
      "outputs": [],
      "source": [
        "input_code = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYd5UilqG-rH",
        "outputId": "01af1581-4c07-45e0-e0af-59bcf7db54a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Your Task is to act as Python Code Explainer.\n",
            "  I'll give you a Code Snippet.\n",
            "  Your Job is to explain the Code Snippet step-by-step.\n",
            "  Break down the code into as many steps as possible.\n",
            "  Share intermediate checkpoints & steps along with results.\n",
            "  Few good examples of python code output between #### seperator:\n",
            "  ####\n",
            "  \n",
            "----------------------------\n",
            "Example 1: Code Snippet\n",
            "x = 10\n",
            "def foo():\n",
            "  global x\n",
            "  x = 5\n",
            "\n",
            "foo()\n",
            "print(x)\n",
            "Correct output: 5\n",
            "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
            "So, print(x) outside the function print the modfied value is 5.\n",
            "-----------------------------\n",
            "\n",
            "Example 2: Code Snippet\n",
            "def modify_list(input_list):\n",
            "  input_list.append(4)\n",
            "  input_list = [1,2,3]\n",
            "my_list = [0]\n",
            "modify_list(my_list)\n",
            "print(my_list)\n",
            "Correct output: [0, 4]\n",
            "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
            "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
            "So, print(my_list) outputs [0, 4].\n",
            "------------------------------\n",
            "\n",
            "  ####\n",
            "  Code Snippet is shared below, delimited with triple backticks:\n",
            "  ```\n",
            "  \n",
            "def my_func(x):\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "print(result)\n",
            "\n",
            "\n",
            "  ```\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "  Your Task is to act as Python Code Explainer.\n",
        "  I'll give you a Code Snippet.\n",
        "  Your Job is to explain the Code Snippet step-by-step.\n",
        "  Break down the code into as many steps as possible.\n",
        "  Share intermediate checkpoints & steps along with results.\n",
        "  Few good examples of python code output between #### seperator:\n",
        "  ####\n",
        "  {python_code_example}\n",
        "  ####\n",
        "  Code Snippet is shared below, delimited with triple backticks:\n",
        "  ```\n",
        "  {input_code}\n",
        "  ```\n",
        "  \"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "SV-DwAGPHKG5",
        "outputId": "b8134c86-b06e-427c-ccc6-2fc051fab582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####\n",
            "  Code Explaination:\n",
            "Step 1: Define a function my_func that takes an integer as input and returns \"High\" if the input is greater than 5, else returns \"Low\".\n",
            "Step 2: Define a variable result and assign it the value of my_func(4) + my_func(6) + my_func(4).\n",
            "Step 3: Print the value of result.\n",
            "\n",
            "Intermediate checkpoints:\n",
            "Step 1: my_func(4) returns \"Low\".\n",
            "Step 2: my_func(6) returns \"High\".\n",
            "Step 3: my_func(4) returns \"Low\".\n",
            "Step 4: result is assigned the value of \"Low\" + \"High\" + \"Low\".\n",
            "Step 5: result is printed and the output is \"LowHighLow\".\n"
          ]
        }
      ],
      "source": [
        "print(get_completion(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8r-Pt9U0_uK"
      },
      "source": [
        "**Gradia App**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-sjjQs-1Q9S",
        "outputId": "a92899b0-2e99-4b0b-bd7e-f2631c63e135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxQnBKh21rp_"
      },
      "outputs": [],
      "source": [
        "# define completion function\n",
        "def get_completion(code_snippet):\n",
        "\n",
        "  python_code_example = f\"\"\"\n",
        "  ----------------------------\n",
        "  Example 1: Code Snippet\n",
        "  x = 10\n",
        "  def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "  foo()\n",
        "  print(x)\n",
        "  Correct output: 5\n",
        "  Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "  So, print(x) outside the function print the modfied value is 5.\n",
        "  -----------------------------\n",
        "\n",
        "  Example 2: Code Snippet\n",
        "  def modify_list(input_list):\n",
        "    input_list.append(4)\n",
        "    input_list = [1,2,3]\n",
        "  my_list = [0]\n",
        "  modify_list(my_list)\n",
        "  print(my_list)\n",
        "  Correct output: [0, 4]\n",
        "  Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "  Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "  So, print(my_list) outputs [0, 4].\n",
        "  ------------------------------\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Your Task is to act as Python Code Explainer.\n",
        "  I'll give you a Code Snippet.\n",
        "  Your Job is to explain the Code Snippet step-by-step.\n",
        "  Break down the code into as many steps as possible.\n",
        "  Share intermediate checkpoints & steps along with results.\n",
        "  Few good examples of python code output between #### seperator:\n",
        "  ####\n",
        "  {python_code_example}\n",
        "  ####\n",
        "  Code Snippet is shared below, delimited with triple backticks:\n",
        "  ```\n",
        "  {code_snippet}\n",
        "  ```\n",
        "  \"\"\"\n",
        "  completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length if the response\n",
        "    max_output_tokens=500,\n",
        "  )\n",
        "  response = completion.result\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtonjN6XAny1"
      },
      "outputs": [],
      "source": [
        "# gradio.Interface(get_completion, \"text\", \"text\").launch(share=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "j_YDHUgZ485y",
        "outputId": "f4792cef-d5c6-44e6-a2a7-9f0013a91845"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0482d0bde4ed3dc869.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0482d0bde4ed3dc869.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/text-bison-001:generateText?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2012.85ms\n"
          ]
        }
      ],
      "source": [
        "# Define app UI\n",
        "import gradio\n",
        "\n",
        "iface = gradio.Interface(fn=get_completion, inputs=[gradio.Textbox(label=\"Insert Code Snippet\")],\n",
        "                     outputs=[gradio.Textbox(label=\"Explanation Here\")],\n",
        "                     title=\"Code Explainer -- Developed by team ByteBlasters for Ideathon'24 at NSEC\",\n",
        "                     description=\"This Projects Analyse and explain code easy and Best work with Python  This Project uses Google Colab , PLAM API and Gradio \",\n",
        "                     allow_flagging=\"never\",)\n",
        "\n",
        "\n",
        "iface.launch(share=True,debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9CmH0Cf6lKD"
      },
      "outputs": [],
      "source": [
        "iface.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeq3GbZd7xvD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}